{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing Minesweeper using Deep Reinforcement Learning\n",
    "##### Oleg Yurchenko (20130857), Alisher Tortay (20140904)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "In our project we decided to play minesweeper game using different Deep Reinforcement Learninig models. So far we examined following models:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Policy based agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Q learning agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Supervised learning network for size of 5x5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game\n",
    "We wrote our own version of minesweeper. In our game there is no mine at position (0,0). Interface works as follows:\n",
    "we open a square using open(pos) function. This functions returns state in the form of array(1,16) (or array(1,64) for 8x8), reward, and whether gane is finished. Reward is 1 if agent opens correct field, -1 if opens mine or already opened field, and 10 if agent opens all non-mine fields. When agents opens mine or already opened field game is finished. State is represented as follows: each field has its own number. If field is covered - 0. Otherwise usual number + 1. Thus, every fiels can have value from 0 to 9. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Policy based agent\n",
    "In this model we used two architectures for neural network.\n",
    "In first one we used feed forward network with one hidden layer with 128 nodes. We used epsilon-greedy exploration strategy. We feed network with whole board (4x4 and 8x8). For 4x4 board with 3 mines after 10,000 training games rate of wins is 2%. We would get same rate with making random moves. Policy loss doesn't decrease. For 8x8 during 20,000 training iterations we agent could win only two games.\n",
    "In other architecture we added two convolutional layers of size 3 and 5. But for some reasonsperformance doesn't improve. For 8x8 during 10,000 training iterations we agent could win only one game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(states, W, b, strides=1):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    states = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    states = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(states)\n",
    "\n",
    "def maxpool2d(states, k=2):\n",
    "    return tf.nn.max_pool(states, ksize=[1, k, k, 1], strides=[1, k, k, 1],padding='SAME')\n",
    "\n",
    "def conv_net(states, weights, biases):\n",
    "    states = tf.reshape(states, shape=[-1, size, size, 1])\n",
    "\n",
    "    conv1 = conv2d(states, weights['wc1'], biases['bc1'])\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out\n",
    "\n",
    "def toHot(inputState):\n",
    "    temp = []\n",
    "    s = np.array([])\n",
    "    for i in range(inputState.shape[1]):\n",
    "        sample = np.zeros(10)\n",
    "        sample[int(inputState[0,i])] = 1.0\n",
    "        temp.append(sample)\n",
    "    return np.asmatrix(np.append(s, temp))\n",
    "\n",
    "def policy_network(states):\n",
    "    weights = {\n",
    "        'wc1': tf.Variable(tf.random_normal([3, 3, 1, state_dim])),\n",
    "        'wc2': tf.Variable(tf.random_normal([5, 5, state_dim, state_dim])),\n",
    "        'wd1': tf.Variable(tf.random_normal([2*2*state_dim, 256])),\n",
    "        'out': tf.Variable(tf.random_normal([256, num_actions]))\n",
    "    }\n",
    "    biases = {\n",
    "        'bc1': tf.Variable(tf.random_normal([state_dim])),\n",
    "        'bc2': tf.Variable(tf.random_normal([state_dim])),\n",
    "        'bd1': tf.Variable(tf.random_normal([256])),\n",
    "        'out': tf.Variable(tf.random_normal([num_actions]))\n",
    "    }\n",
    "\n",
    "    return conv_net(states, weights, biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Q learning agent\n",
    "In this model we changed usual q learning algorith to fit our problem. We are not interested in long-term reward. Instead we are more interested in the immediate reward. Thus we removed future rewards. We also used table with true positions of mines to produce our target Q values. During 10,000 training games on board 4x4 with 3 mines agent managed to win 900 games. Since minesweeper has strong local dependicies we decided to add convolutional layers. However it had no positive effect on performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Convolution2D, Flatten, Dense, Input\n",
    "from keras import backend as K\n",
    "import keras.callbacks\n",
    "import numpy as np\n",
    "\n",
    "class NN:\n",
    "    def __init__(self, bsize = (4, 4), gamma = 0.99):\n",
    "        self.model = Sequential()\n",
    "        '''\n",
    "        self.model.add(Convolution2D(nb_filter=bsize[0]*bsize[1], nb_row=3, nb_col=3, activation='relu', border_mode='same', input_shape=(1, bsize[0], bsize[1]), init='uniform'))\n",
    "        self.model.add(\n",
    "            Convolution2D(nb_filter=bsize[0] * bsize[1], nb_row=3, nb_col=3, activation='relu', border_mode='same',\n",
    "                          input_shape=(1, bsize[0], bsize[1]), init='uniform'))\n",
    "        self.model.add(Flatten())\n",
    "        '''\n",
    "        self.model.add(Dense(200, activation='relu', init='uniform', input_dim=bsize[0]*bsize[1]*10))\n",
    "        self.model.add(Dense(200, activation='relu', init='uniform'))\n",
    "        self.model.add(Dense(100, activation='linear', init='uniform'))\n",
    "        self.model.add(Dense(bsize[0]*bsize[1], activation='softmax', init='uniform'))\n",
    "        opt = keras.optimizers.Adam(lr=0.001)\n",
    "        self.model.compile(loss='mean_squared_error', optimizer=opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Supervised learning network for size of 5x5\n",
    "In this model we used neural network with 3 hidden layers. It takes board of 1x25 as input and produces scalar output between 0 and 1. Output is, in some sense, probability that central square of 5x5 is free of mines.\n",
    "We trained the network using true mine positions and deployed it on every bordering square (this way agent cannot open already opened fields). We also changed representation of state: in each field, instead of one number we use vector of length 10 with only one value equal to 1 and others are 0. We should notice that this model scales easily because we always train the same network. After training our model on 10,000 games on board 8x8 with 8 mines, we test its performance on two boards: 8x8 with 8 mines and 10x10 with 10 mines. Winning rates are 25% and 35%, respectively. We can see that even though we increased the board size, winnig rate also increases. This happened because in second case density of mines is lower (10% compared to 12.5%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from game import *\n",
    "from ExperienceBuf import *\n",
    "from NN import *\n",
    "import numpy as np\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, load = False):\n",
    "        self.bsize = (10, 10)\n",
    "        self.game = Game(10, 10, 10)\n",
    "        self.nn = NN(self.bsize)\n",
    "        if load:\n",
    "            self.nn.load()\n",
    "        self.buf = ExperienceBuf()\n",
    "        self.num_episodes = 5000\n",
    "        self.max_game_moves = 64\n",
    "        self.observe = 10\n",
    "        self.e = 0.0\n",
    "        self.y = 0.99\n",
    "        self.mini_batch = 64\n",
    "\n",
    "    def toHot(self, inputState):\n",
    "        temp = []\n",
    "        s = np.array([])\n",
    "        for i in range(25):\n",
    "            sample = np.zeros(10)\n",
    "            sample[int(inputState[i])] = 1.0\n",
    "            temp.append(sample)\n",
    "        return np.asmatrix(np.append(s, temp))\n",
    "\n",
    "    def learn(self):\n",
    "        count = 0\n",
    "\n",
    "        for i in range(self.num_episodes):\n",
    "            self.game.reset()\n",
    "            _, _, d = self.game.open((0, 0))\n",
    "            while (not d):\n",
    "                pmines, true_vals = self.choose_possible_mines()\n",
    "\n",
    "                states = self.make_states(pmines)\n",
    "                states.shape = (len(pmines), 1, 25)\n",
    "\n",
    "                pboard = np.zeros(self.bsize)\n",
    "\n",
    "                for i in range(len(pmines)):\n",
    "                    tmp_state = states[i].flatten()\n",
    "                    tmp_state = self.toHot(tmp_state)\n",
    "                    self.buf.memorize(tmp_state, true_vals[i])\n",
    "                    h, w = pmines[i]\n",
    "                    pboard[h, w] = self.nn.predict(tmp_state)\n",
    "\n",
    "                #print pmines\n",
    "                #print states\n",
    "                #print pboard\n",
    "\n",
    "                act = np.argmax(pboard.flatten())\n",
    "                a = (act // self.bsize[1], act % self.bsize[1])\n",
    "\n",
    "                if np.random.rand(1) < self.e:\n",
    "                    a = (np.random.randint(self.bsize[0]), np.random.randint(self.bsize[1]))\n",
    "\n",
    "\n",
    "                #print a\n",
    "                _, _, d = self.game.open(a)\n",
    "\n",
    "                count += 1\n",
    "\n",
    "            if count > self.observe:\n",
    "                x, y, bsize = self.buf.get_batch(self.mini_batch)\n",
    "                x = np.asarray(x)\n",
    "                x.shape = (bsize, 250)\n",
    "                y = np.asarray(y)\n",
    "                y.shape = (bsize, 1)\n",
    "                self.nn.train(x, y)\n",
    "\n",
    "            if count % 50 == 0:\n",
    "                self.nn.save()\n",
    "\n",
    "\n",
    "    def choose_possible_mines(self):\n",
    "        pmines = []\n",
    "        true_vals = []\n",
    "        s = self.game.display_board\n",
    "        for h in range(self.bsize[0]):\n",
    "            for w in range(self.bsize[1]):\n",
    "                if s[h, w] == COVERED:\n",
    "                    flag = False\n",
    "                    for i in range(-1, 2):\n",
    "                        h_new = h + i\n",
    "                        if (h_new >= 0):\n",
    "                            for j in range(-1, 2):\n",
    "                                w_new = w + j\n",
    "                                if (w_new >= 0):\n",
    "                                    try:\n",
    "                                        if (s[h_new, w_new] > 0):\n",
    "                                            flag = True\n",
    "                                    except:\n",
    "                                        pass\n",
    "                                if flag: break\n",
    "                        if flag: break\n",
    "\n",
    "                    if flag:\n",
    "                        pmines.append((h, w))\n",
    "                        if self.game.board[h, w] == BOMB: true_vals.append(0)\n",
    "                        else:true_vals.append(1)\n",
    "        return pmines, true_vals\n",
    "\n",
    "    def make_states(self, border):\n",
    "        states = np.zeros((len(border), 5, 5))\n",
    "        s = self.game.display_board + 1\n",
    "        for k in range(len(border)):\n",
    "            h, w = border[k]\n",
    "            for i in range(0, 5):\n",
    "                h_new = h + i - 2\n",
    "                for j in range(0, 5):\n",
    "                    w_new = w + j - 2\n",
    "                    if (h_new >= 0 and w_new >= 0):\n",
    "                        try:\n",
    "                            states[k, i, j] = s[h_new, w_new]\n",
    "                        except:\n",
    "                            pass\n",
    "        return states\n",
    "\n",
    "\n",
    "    def play(self):\n",
    "        for i in range(self.num_episodes):\n",
    "            self.game.reset()\n",
    "            _, _, d = self.game.open((0, 0))\n",
    "            while (not d):\n",
    "                pmines, true_vals = self.choose_possible_mines()\n",
    "\n",
    "                states = self.make_states(pmines)\n",
    "                states.shape = (len(pmines), 1, 25)\n",
    "\n",
    "                pboard = np.zeros(self.bsize)\n",
    "\n",
    "                for i in range(len(pmines)):\n",
    "                    tmp_state = states[i].flatten()\n",
    "                    tmp_state = self.toHot(tmp_state)\n",
    "                    self.buf.memorize(tmp_state, true_vals[i])\n",
    "                    h, w = pmines[i]\n",
    "                    pboard[h, w] = self.nn.predict(tmp_state)\n",
    "\n",
    "                #print pmines\n",
    "                #print states\n",
    "                #print pboard\n",
    "\n",
    "                act = np.argmax(pboard.flatten())\n",
    "                a = (act // self.bsize[1], act % self.bsize[1])\n",
    "                _, _, d = self.game.open(a)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
